import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator


fsize = 15
tsize = 15
tdir = 'in'
major = 5.0
minor = 3.0
style = 'default'
lwidth = 0.5
lhandle = 3.0

plt.style.use(style)
plt.rcParams.update(plt.rcParamsDefault)
#plt.rcParams['text.usetex'] = True
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = fsize
plt.rcParams['legend.fontsize'] = tsize
plt.rcParams['xtick.direction'] = tdir
plt.rcParams['ytick.direction'] = tdir
plt.rcParams['xtick.major.size'] = major
plt.rcParams['xtick.minor.size'] = minor
plt.rcParams['ytick.major.size'] = major
plt.rcParams['ytick.minor.size'] = minor
plt.rcParams['axes.linewidth'] = lwidth
plt.rcParams['legend.handlelength'] = lhandle


########################################################################
# English
########################################################################
# BERT-base-uncased
########################################################################
title = 'BERT-base-uncased'
train_0_01 = [0.6670863812934525, 0.7279276521899394, 0.7521322658828504, 0.7694349361328888, 0.7784602350525042, 0.7811061279783081]
validation_0_01 = [0.7724997537508943 ,0.7933758909335401 ,0.8040932880094456 ,0.8074276337268831 ,0.810817984821049 ,0.812024127834295]
test_0_01 = 0.7594756489046671
train_0_005 = [0.6782729681792927, 0.7276760038208191, 0.7520256042465717, 0.7665433580224204, 0.7767568474645611, 0.7793812999842847]
validation_0_005 = [0.7841141265347802 ,0.8028004472595911 ,0.8051851464141943 ,0.8058985668353588 ,0.8082816519386372 ,0.8089667130808331]
test_0_005 = 0.7499771084578993
train_0_001 = [0.6295178463759806, 0.7164170105010886, 0.7439169495333177, 0.7585925254980911, 0.7682028384124757, 0.7706709560269178]
validation_0_001 = [0.7647324494582103 ,0.7864160185020919 ,0.7976775166779387 ,0.7970589237243507 ,0.8015285015419501 ,0.8013391455393872]
test_0_001 = 0.7530739129462626
train_0_0005 = [0.6585387741469894, 0.7344295089448608, 0.7481653150302372, 0.763060853248611, 0.7718778580978425, 0.7754874374917793]
validation_0_0005 = [0.7796803385808362 ,0.7988387363405186 ,0.800053948922922 ,0.8027205308343331 ,0.8040517310412648 ,0.8049082725576401]
test_0_0005 = 0.7565474507901706
train_0_0001 = [0.6662209552534816, 0.7269999250196131, 0.7476775707469635, 0.7628557633725177, 0.770138746315303, 0.774274483169599]
validation_0_0001 = [0.7734572437355118 ,0.795097514897452 ,0.8024075306183283 ,0.8016611333060786 ,0.8048844540358647 ,0.8055636182056307]
test_0_0001 = 0.7468513979945739

########################################################################
# NLI-DistilRoBERTa-base-v2
########################################################################
# title = 'NLI-DistilRoBERTa-base-v2'
# train_0_01 = [0.8290864879124278, 0.8353707743268753, 0.8395399350534166, 0.8423213696175129, 0.853713160769086, 0.8559203849619771]
# validation_0_01 = [0.8598924599690069 ,0.8606947719105913 ,0.8607906296279892 ,0.8574569671850446 ,0.865107862144724 ,0.8658448039344342]
# test_0_01 = 0.8266299088716405
# train_0_005 = [0.8304337407991339, 0.8313192367797438, 0.8405294904552997, 0.8543161534119825, 0.8591214012984705, 0.8615178633369197]
# validation_0_005 = [0.8572025132727883 ,0.8461356995856399 ,0.8509670321012889 ,0.8574963070698446 ,0.8606734943158362 ,0.8620088955932829]
# test_0_005 = 0.831851310717841
# train_0_001 = [0.8204965674599016, 0.8314535291774998, 0.8442080385270145, 0.8502112538668869, 0.8572557281011274, 0.8596072880182574]
# validation_0_001 = [0.8530922613259393 ,0.8506573450548304 ,0.855549764801248 ,0.858578202834266 ,0.8607018395311271 ,0.8613571869584783]
# test_0_001 = 0.83085471663051
# train_0_0005 = [0.8270109770338161, 0.8433791790508774, 0.8463726478462925, 0.8550342754439839, 0.8622430330502365, 0.86403878199774]
# validation_0_0005 = [0.8558642294310749 ,0.8581723555790975 ,0.8569167479510235 ,0.8630228375741538 ,0.8657647807475368 ,0.8659513556093902]
# test_0_0005 = 0.838209791649207
# train_0_0001 = [0.8295993741012915, 0.8349999128302138, 0.8435203197895009, 0.8552627674506703, 0.8602256974199041, 0.8617117018985881]
# validation_0_0001 = [0.857475803496976 ,0.8550980580838267 ,0.8559550612154616 ,0.861738831349969 ,0.8637318576364098 ,0.8632143732935232]
# test_0_0001 = 0.8330653211910718

########################################################################
# microsoft/deberta-base
########################################################################
# title = 'DeBERTa-base'
# train_0_01 = [0.30560017131884526, 0.3875608832082273, 0.4130736028002965, 0.4366776645897045, 0.48722039729632133, 0.5070445800769076]
# validation_0_01 = [0.3728385585343933 ,0.4904238751279528 ,0.5110085467904272 ,0.5677722229481665 ,0.5736831848555812 ,0.5880943349774018]
# test_0_01 = 0.48409141472201583
# train_0_005 = [0.5701702406996685, 0.5777264456692957, 0.553410898127481, 0.5264251337638429, 0.5578103580197659, 0.5715064502468155]
# validation_0_005 = [0.68360174816922 ,0.7200293693171641 ,0.6499136119014087 ,0.6161421763016555 ,0.6566015915833455 ,0.671200400945996]
# test_0_005 = 0.5872668067810977
# train_0_001 = [0.5143657873441524, 0.5069279960448502, 0.6390070762788178, 0.6431716767090774, 0.6619296678040822, 0.6636555286735317]
# validation_0_001 = [0.6808732100913923 ,0.7323937847921982 ,0.7454553044822491 ,0.7592833227071477 ,0.7695979688301919 ,0.7699426814962476]
# test_0_001 = 0.687763354197377
# train_0_0005 = [0.571945917019534, 0.6605819220888484, 0.6807420619186718, 0.6826164812997771, 0.6882684957134625, 0.6891185781303077]
# validation_0_0005 = [0.7438412695605571 ,0.7811494593121099 ,0.788456389317899 ,0.7870485876346162 ,0.7905836274407329 ,0.7899069775075773]
# test_0_0005 = 0.7016700312051531
# train_0_0001 = [0.5383549562278817, 0.6359476876744846, 0.6651095189879108, 0.6809488023969503, 0.6984567278081765, 0.7015114001299949]
# validation_0_0001 = [0.6734550483884099 ,0.7669775595485101 ,0.7708965782673016 ,0.7862219180249768 ,0.7945409636182048 ,0.7964961937986382]
# test_0_0001 = 0.7159522229078484

########################################################################
# 
########################################################################
# title = ''
# train_0_01 = []
# validation_0_01 = []
# test_0_01 = 0


########################################################################
# Polski
########################################################################
# 
########################################################################



########################################################################
# Plotting part
########################################################################


x = range(1, len(train_0_01)+1)
# scalling from <0,1> to range <0, 100>
train_0_01 = [i*100 for i in train_0_01]
validation_0_01 = [i*100 for i in validation_0_01]
train_0_005 = [i*100 for i in train_0_005]
validation_0_005 = [i*100 for i in validation_0_005]
train_0_001 = [i*100 for i in train_0_001]
validation_0_001 = [i*100 for i in validation_0_001]
train_0_0005 = [i*100 for i in train_0_0005]
validation_0_0005 = [i*100 for i in validation_0_0005]
train_0_0001 = [i*100 for i in train_0_0001]
validation_0_0001 = [i*100 for i in validation_0_0001]


xsize = 8
ysize = 5
plt.figure( figsize=(xsize, ysize) )

plt.plot( x, train_0_01, '-o', label=r'$\lambda$ = 0.01, trening', lw=1, ms=4 , c='#DEEDCF' )
plt.plot( x, validation_0_01, '-h', label=r'$\lambda$ = 0.01, walidacja', lw=1, ms=4 , c='#FFFFA8' )
plt.plot( x, train_0_005, '-o', label=r'$\lambda$ = 0.005, trening', lw=1, ms=4 , c='#56B870' )
plt.plot( x, validation_0_005, '-h', label=r'$\lambda$ = 0.005, walidacja', lw=1, ms=4 , c='#FFAB49' )
plt.plot( x, train_0_001, '-o', label=r'$\lambda$ = 0.001, trening', lw=1, ms=4 , c='#188977' )
plt.plot( x, validation_0_001, '-h', label=r'$\lambda$ = 0.001, walidacja', lw=1, ms=4 , c='#F8601A' )
plt.plot( x, train_0_0005, '-o', label=r'$\lambda$ = 0.0005, trening', lw=1, ms=4 , c='#2152bc' )
plt.plot( x, validation_0_0005, '-h', label=r'$\lambda$ = 0.0005, walidacja', lw=1, ms=4 , c='#D21E05' )
plt.plot( x, train_0_0001, '-o', label=r'$\lambda$ = 0.0001, trening', lw=1, ms=4 , c='#B42A78' )
plt.plot( x, validation_0_0001, '-h', label=r'$\lambda$ = 0.0001, walidacja', lw=1, ms=4 , c='#703C16' )

ax = plt.gca()
ax.xaxis.set_minor_locator( MultipleLocator(1) )
ax.yaxis.set_minor_locator( MultipleLocator(.5) )

plt.xlabel( 'Liczba epok', labelpad=10 )
plt.xticks( x )
plt.ylabel( 'Dokładność (accuracy), %', labelpad=20 )
plt.title( title, pad=10 )
plt.legend( loc='best' )
    
plt.show()


